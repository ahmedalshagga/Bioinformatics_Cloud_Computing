###RNA-SEQ pipeline

This pipeline does a series of analyses, including quality control, index construction, read alignment, data processing, quantification, and differential expression analysis.

Script 1 quality control:

```{bash}
# Preliminary Quality Check: assess the quality of raw RNA-seq reads

# Initialize conda to enable conda commands in the current shell
eval "$(conda shell.bash hook)"

# Activate the conda environment containing FastQC and MultiQC
conda activate Quality_control_Env

# Define the directory containing raw FASTQ files
raw_dir="raw_data"

# Define the output directory for individual FastQC results
FASTQC_OUT="processed_data/fastqc"

# Define the output directory for the aggregated MultiQC report
MULTIQC_OUT="processed_data/multiqc"

# Create output directories if they do not already exist
echo "--> Creating output directories..."
mkdir -p "$FASTQC_OUT"
mkdir -p "$MULTIQC_OUT"

# Print message indicating the start of FastQC analysis
echo "Running FASTQC files in $raw_dir..."

# Run FastQC on all compressed FASTQ files in the raw data directory
fastqc "$raw_dir"/*.fastq.gz -o "$FASTQC_OUT"

# Print message indicating the start of MultiQC aggregation
echo "Running MultiQC aggregation..."

# Aggregate all FastQC reports into a single MultiQC summary report
multiqc "$FASTQC_OUT" -o "$MULTIQC_OUT"

# Print message indicating cleanup of intermediate FastQC files
echo "--> Deleting raw FastQC files to save disk space..."

# Remove FastQC-generated zip files after MultiQC aggregation
rm -rf "$FASTQC_OUT"/*.zip

# Output the location of the final MultiQC HTML report
echo "Report available in $MULTIQC_OUT/multiqc_report.html"
```

This script was used to perform preliminary quality assessment of the raw sequencing reads. The workflow first initialized a dedicated Conda environment containing all required quality-control tools. The script defined the directories for the raw FASTQ files and the output folders used to store the FastQC and MultiQC results. FastQC was then executed on all .fastq.gz files in the raw-data directory to evaluate sequencing quality metrics, including per-base quality scores, GC content, duplication levels, and adapter contamination.

Following individual FastQC analyses, MultiQC was run to aggregate all FastQC outputs into a single comprehensive report, allowing efficient visualization and comparison of read-quality statistics across all samples. After aggregation, the temporary FastQC .zip files were removed to reduce disk usage. The final quality-control summary was saved as an HTML report generated by MultiQC.

Script 2 index construction:

```{bash}
#!/bin/bash
# Specify the script interpreter as bash

#SBATCH --partition=msc_appbio
#SBATCH --job-name=build_index
#SBATCH --output=logs/index_%j.output
#SBATCH --error=logs/index_%j.error
#SBATCH --nodes=1
#SBATCH --time=02:00:00
#SBATCH --mem=8G
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4

# Enable conda commands in the current shell session
eval "$(conda shell.bash hook)"

# Activate the conda environment containing Bowtie2 and gffread
conda activate isobutanol_paper

# Define the main reference directory
REF_DIR="References"

# Define the path to the original reference genome FASTA file
ORIGINAL_FASTA="${REF_DIR}/S288C_reference_sequence_R64-1-1_20110203.fsa"

# Define the path for the cleaned FASTA file with renamed chromosomes
CLEAN_FASTA="${REF_DIR}/genome_clean.fa"

# Define the directory for the Bowtie2 index files
INDEX_DIR="${REF_DIR}/BowtieIndex"

# Define the prefix name for the Bowtie2 index
INDEX_PREFIX="${INDEX_DIR}/genome"

# Define the path to the original GFF annotation file
ORIGINAL_GFF="${REF_DIR}/saccharomyces_cerevisiae_R64-1-1_20110208.gff"

# Define the output path for the converted GTF annotation file
NEW_GTF="${REF_DIR}/genes.gtf"

# This step standardizes chromosome names from NCBI format
# (e.g. ref|NC_001133|) to SGD format (e.g. chrI)
# to ensure consistency between the genome FASTA and annotation files

echo "Renaming chromosomes in FASTA..."

# Check that the original FASTA file exists before processing
if [ -f "$ORIGINAL_FASTA" ]; then

    # Replace each NCBI chromosome identifier with the corresponding SGD name
    sed 's/^>ref|NC_001133|.*/>chrI/' "$ORIGINAL_FASTA" | \
    sed 's/^>ref|NC_001134|.*/>chrII/' | \
    sed 's/^>ref|NC_001135|.*/>chrIII/' | \
    sed 's/^>ref|NC_001136|.*/>chrIV/' | \
    sed 's/^>ref|NC_001137|.*/>chrV/' | \
    sed 's/^>ref|NC_001138|.*/>chrVI/' | \
    sed 's/^>ref|NC_001139|.*/>chrVII/' | \
    sed 's/^>ref|NC_001140|.*/>chrVIII/' | \
    sed 's/^>ref|NC_001141|.*/>chrIX/' | \
    sed 's/^>ref|NC_001142|.*/>chrX/' | \
    sed 's/^>ref|NC_001143|.*/>chrXI/' | \
    sed 's/^>ref|NC_001144|.*/>chrXII/' | \
    sed 's/^>ref|NC_001145|.*/>chrXIII/' | \
    sed 's/^>ref|NC_001146|.*/>chrXIV/' | \
    sed 's/^>ref|NC_001147|.*/>chrXV/' | \
    sed 's/^>ref|NC_001148|.*/>chrXVI/' | \
    sed 's/^>ref|NC_001224|.*/>chrmt/' | \
    sed 's/^>ref|NC_001398|.*/>2-micron/' > "$CLEAN_FASTA"

    # Confirm creation of the cleaned FASTA file
    echo "Created clean FASTA at $CLEAN_FASTA"
fi

# Print status message before starting index construction
echo "Building Bowtie2 index..."

# Create the index directory if it does not already exist
mkdir -p "$INDEX_DIR"

# Build the Bowtie2 index using four CPU threads
bowtie2-build --threads 4 "$CLEAN_FASTA" "$INDEX_PREFIX"

# Confirm completion of index building
echo "Index building complete."

# Tophat requires gene annotations in GTF format
# gffread is used to convert the original GFF file to GTF

echo "Converting GFF to GTF..."

# Convert the annotation file from GFF to GTF format
gffread "$ORIGINAL_GFF" -T -o "$NEW_GTF"

# Confirm creation of the GTF annotation file
echo "Created GTF file at $NEW_GTF"

# Indicate successful completion of all reference preparation steps
echo "All reference preparation tasks completed successfully."
```

This script initialized a dedicated Conda environment containing Bowtie2 and other required tools.

First, the script defined the directory structure and input files, including the original FASTA genome sequence and the corresponding GFF annotation file. Chromosome identifiers in the FASTA file were then standardized to match the Saccharomyces Genome Database (SGD) naming convention (e.g., chrI, chrII, …, chrXVI, and chrmt). This renaming step was performed using a series of sed commands and ensured consistent chromosome naming between the genome FASTA and gene-annotation files.

After generating the cleaned FASTA file, a Bowtie2 index was constructed using bowtie2-build with four CPU threads. The resulting index files were stored in a dedicated index directory for downstream alignment.

Finally, the script converted the original GFF annotation into GTF format using gffread, as some RNA-seq downstream tools require GTF rather than GFF. The resulting genes.gtf file and the Bowtie2 index together formed the complete reference package used for subsequent read alignment and quantification.

Script 3 read alignment:

```{bash}
#!/bin/bash
# SLURM job configuration for RNA-seq alignment

#SBATCH --partition=msc_appbio
#SBATCH --job-name=yeast_alignment
#SBATCH --output=logs/align_%j.output
#SBATCH --error=logs/align_%j.error
#SBATCH --nodes=1
#SBATCH --time=08:00:00
#SBATCH --mem=8G
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4

# Enable conda commands in the current shell
eval "$(conda shell.bash hook)"

# Activate the conda environment for alignment
conda activate isobutanol_paper

# Define the directory containing raw FASTQ files
RAW_DIR="raw_data"

# Define the Bowtie2 index prefix
REF_INDEX="References/BowtieIndex/genome"

# Define the output directory for TopHat results
OUT_DIR="processed_data/tophat"

# Define the gene annotation file in GTF format
GTF_FILE="References/genes.gtf"

# Define the directory for consolidated BAM files
BAM_DIR="processed_data/bams"

# Create output directories if they do not exist
mkdir -p "$OUT_DIR"
mkdir -p "$BAM_DIR"
mkdir -p logs

# Loop through all paired-end read 1 files
for file in "$RAW_DIR"/*_1.fastq.gz; do

    # Extract the sample name from the filename
    filename=$(basename "$file")
    sample_id="${filename%_1.fastq.gz}"

    # Print the current sample being processed
    echo "Processing sample: $sample_id"

    # Define read 1 and read 2 file paths
    r1="${RAW_DIR}/${sample_id}_1.fastq.gz"
    r2="${RAW_DIR}/${sample_id}_2.fastq.gz"

    # Define the output directory for the current sample
    sample_out="${OUT_DIR}/${sample_id}"
    mkdir -p "$sample_out"

    # Run TopHat for spliced alignment
    tophat -p 4 \
           -G "$GTF_FILE" \
           -o "$sample_out" \
           "$REF_INDEX" \
           "$r1" "$r2"

    # Confirm completion of alignment
    echo "Finished mapping $sample_id"

    # Copy the accepted BAM file to a central directory
    if [ -f "$sample_out/accepted_hits.bam" ]; then
        cp "$sample_out/accepted_hits.bam" "$BAM_DIR/${sample_id}.bam"
        echo "--> Saved BAM file to $BAM_DIR/${sample_id}.bam"
    fi

done

# Print completion message
echo "Mapping of all samples completed"
```

Also，The script initialized a dedicated Conda environment containing Tophat and its dependencies. And, defined the input directories for raw FASTQ files, the Bowtie2 reference index, the gene-annotation GTF file, and the output directories for Tophat results and final BAM files. It then iterated through all paired-end read files in the raw-data directory by detecting files ending in _1.fastq.gz. For each sample, the script extracted the sample ID, located the corresponding read 1 and read 2 files, and created a sample-specific output folder.

Tophat was executed for each sample using four CPU threads, with the GTF annotation file provided to guide spliced alignment. The cleaned reference genome index generated earlier was used as the alignment target. Upon completion, Tophat produced the standard alignment file accepted_hits.bam.

If the BAM file was successfully generated, it was copied into a centralized directory (processed_data/bams/) using the sample’s name, ensuring consistent file organization for downstream analyses. This loop continued until all samples were processed, after which the script reported the completion of the alignment step.

Script 4 data processing:

```{bash}
#!/bin/bash
# SLURM job configuration for BAM processing

#SBATCH --partition=msc_appbio
#SBATCH --job-name=bam_processing
#SBATCH --output=logs/processing_%j.output
#SBATCH --error=logs/processing_%j.error
#SBATCH --nodes=1
#SBATCH --time=08:00:00
#SBATCH --mem=16G
#SBATCH --ntasks=1

# Enable conda commands in the current shell
eval "$(conda shell.bash hook)"

# Activate the conda environment containing Picard
conda activate isobutanol_paper

# Define the directory containing input BAM files
INPUT_DIR="processed_data/bams"

# Define the directory for processed BAM files
OUTPUT_DIR="processed_data/processed_bams"

# Create output directories if they do not exist
mkdir -p "$OUTPUT_DIR"
mkdir -p logs

# Loop through all BAM files
for bam_file in "$INPUT_DIR"/*.bam; do

    # Extract the sample name from the BAM filename
    filename=$(basename "$bam_file")
    sample_id="${filename%.bam}"

    # Print the current sample being processed
    echo "Processing sample: $sample_id"

    # Define intermediate and final BAM file paths
    sorted_bam="${OUTPUT_DIR}/${sample_id}_sorted.bam"
    dedup_bam="${OUTPUT_DIR}/${sample_id}_deduplicated.bam"
    metrics_file="${OUTPUT_DIR}/${sample_id}_metrics.txt"

    # Sort BAM file by genomic coordinates
    picard SortSam \
        I="$bam_file" \
        O="$sorted_bam" \
        SORT_ORDER=coordinate \
        VALIDATION_STRINGENCY=LENIENT

    # Remove PCR duplicates from the sorted BAM file
    picard MarkDuplicates \
        I="$sorted_bam" \
        O="$dedup_bam" \
        M="$metrics_file" \
        REMOVE_DUPLICATES=true \
        VALIDATION_STRINGENCY=LENIENT

    # Remove intermediate sorted BAM file
    rm "$sorted_bam"

    # Confirm completion for the current sample
    echo "Finished processing $sample_id"

done

# Print completion message
echo "All samples processed successfully"
```

This script was used to perform standardized post-alignment processing of RNA-seq BAM files prior to downstream quantification. Job parameters were defined through the scheduler, and a Conda environment containing Picard was activated at runtime.

The script specified directories for input BAM files (generated from Tophat) and output processed files. For each BAM file within the input directory, the script extracted the sample identifier and generated filenames for sorted BAMs, deduplicated BAMs, and Picard metrics logs.

Each BAM file first underwent coordinate sorting using Picard's SortSam tool, producing a sorted BAM file suitable for further processing. Following sorting, duplicate reads were identified and removed using MarkDuplicates with the REMOVE_DUPLICATES=true option. A metrics file was generated for each sample to document duplication rates and provide traceability for removed reads. After deduplication, the intermediate sorted BAM file was deleted to conserve storage space.

The final deduplicated BAM files, suitable for quantification or other downstream analyses, were saved in the designated output directory. The script iterated over all input samples and reported completion upon processing all available BAM files.

Script 5 quantification:

```{bash}
#!/bin/bash
# SLURM job configuration for transcript quantification

#SBATCH --partition=msc_appbio
#SBATCH --job-name=cufflinks_quant
#SBATCH --output=logs/quantification_%j.output
#SBATCH --error=logs/quantification_%j.error
#SBATCH --nodes=1
#SBATCH --time=08:00:00
#SBATCH --mem=8G
#SBATCH --ntasks=4

# Enable conda commands in the current shell
eval "$(conda shell.bash hook)"

# Activate the conda environment containing Cufflinks
conda activate isobutanol_paper

# Define the directory containing processed BAM files
INPUT_DIR="processed_data/processed_bams"

# Define the output directory for Cufflinks results
OUTPUT_DIR="processed_data/cufflinks"

# Define the reference annotation file
REF_ANNOTATION="References/genes.gtf"

# Define the mask file for rRNA and tRNA
MASK_FILE="References/rRNA_tRNA_mask.gtf"

# Create output directories if they do not exist
mkdir -p "$OUTPUT_DIR"
mkdir -p logs

# Create a mask file containing rRNA and tRNA annotations
echo "Creating rRNA/tRNA mask file..."
grep -E 'rRNA|tRNA' "$REF_ANNOTATION" > "$MASK_FILE"

# Loop through all deduplicated BAM files
for bam_file in "$INPUT_DIR"/*_deduplicated.bam; do

    # Extract the sample name from the BAM filename
    filename=$(basename "$bam_file")
    sample_id="${filename%_deduplicated.bam}"

    # Print the current sample being quantified
    echo "Quantifying sample: $sample_id"

    # Define the output directory for the current sample
    sample_out="${OUTPUT_DIR}/${sample_id}"
    mkdir -p "$sample_out"

    # Run Cufflinks for transcript quantification
    cufflinks -p 4 \
              -G "$REF_ANNOTATION" \
              -M "$MASK_FILE" \
              -o "$sample_out" \
              "$bam_file"

    # Confirm completion for the current sample
    echo "Finished $sample_id"

done

# Print completion message
echo "Quantification step complete"
```

This script was used to generate transcript abundance estimates from the processed RNA-seq BAM files.  The Cufflinks suite was activated prior to quantification.

The script began by defining directory paths for the input BAM files, the output quantification directory, the reference GTF annotation file, and a mask file used to exclude non-coding RNA features. A mask file containing rRNA and tRNA annotations was created by extracting all corresponding entries from the reference GTF using a regular-expression search. This mask file prevents highly abundant rRNA/tRNA reads from inflating transcript-expression estimates.

For each deduplicated BAM file, the script extracted the sample identifier and created a dedicated output directory. Cufflinks was then executed with four threads, using the reference GTF file to guide transcript assembly and quantification. The -G option constrained the analysis to known gene models, and the -M option applied the rRNA/tRNA mask to exclude unwanted alignments. The resulting transcript FPKM estimates, along with associated tracking and model files, were written to the sample-specific output directory.

Script 6 differential expression analysis:

```{bash}
#!/bin/bash
# SLURM job configuration for differential expression analysis

#SBATCH --partition=msc_appbio
#SBATCH --job-name=cuffdiff_de
#SBATCH --output=logs/DE_analysis_%j.output
#SBATCH --error=logs/DE_analysis_%j.error
#SBATCH --nodes=1
#SBATCH --time=12:00:00
#SBATCH --mem=16G
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4

# Enable conda commands in the current shell
eval "$(conda shell.bash hook)"

# Activate the conda environment containing Cuffdiff
conda activate isobutanol_paper

# Define the directory containing processed BAM files
BAM_DIR="processed_data/processed_bams"

# Define the output directory for Cuffdiff results
OUTPUT_DIR="processed_data/cuffdiff"

# Define the reference GTF annotation file
REF_GTF="References/genes.gtf"

# Define the mask file for non-coding RNAs
MASK_FILE="References/rRNA_tRNA_mask.gtf"

# Create output directories if they do not exist
mkdir -p "$OUTPUT_DIR"
mkdir -p logs

# Create a mask file containing non-coding RNA annotations
grep -iE 'rRNA|tRNA|snoRNA|snRNA|ncRNA' "$REF_GTF" > "$MASK_FILE"

# Define BAM files for each experimental condition
GLN3_NO_ISO="${BAM_DIR}/ERR3450094_deduplicated.bam,${BAM_DIR}/ERR3450095_deduplicated.bam"
GLN3_ISO="${BAM_DIR}/ERR3450096_deduplicated.bam,${BAM_DIR}/ERR3450097_deduplicated.bam"
WT_NO_ISO="${BAM_DIR}/ERR3450098_deduplicated.bam,${BAM_DIR}/ERR3450099_deduplicated.bam"
WT_ISO="${BAM_DIR}/ERR3450100_deduplicated.bam,${BAM_DIR}/ERR3450101_deduplicated.bam"

# Print message indicating start of differential expression analysis
echo "Starting differential expression analysis with Cuffdiff"

# Run Cuffdiff with four experimental conditions
cuffdiff \
    -p 4 \
    -o "$OUTPUT_DIR" \
    -L gln3_NoIso,gln3_Iso,WT_NoIso,WT_Iso \
    -M "$MASK_FILE" \
    --library-type fr-unstranded \
    --no-update-check \
    "$REF_GTF" \
    "$GLN3_NO_ISO" "$GLN3_ISO" "$WT_NO_ISO" "$WT_ISO"

# Print completion message
echo "Differential expression analysis complete"
```

This Script was conducted using Cuffdiff. Aligned, deduplicated BAM files were organized into four biological conditions (GLN3_NoIso, GLN3_Iso, WT_NoIso, WT_Iso), each containing replicates. To prevent inflated expression estimates from highly abundant non-coding RNAs, a mask file containing rRNA, tRNA, snoRNA, snRNA, and ncRNA annotations was generated and applied during analysis. Cuffdiff was run in reference-guided mode using the curated GTF annotation and an fr-unstranded library setting. The resulting differential expression statistics were reported in the default output files, including gene_exp.diff.
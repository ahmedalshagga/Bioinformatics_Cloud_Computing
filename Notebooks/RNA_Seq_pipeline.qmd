###RNA-SEQ pipeline

This pipeline does a series of analyses, including quality control, index construction, read alignment, data processing, quantification, and differential expression analysis.

Script 1 quality control:

```{bash}
#Preliminary Quality Check

#Initalising conda enviroment
eval "$(conda shell.bash hook)"
conda activate Quality_control_Env

#Defining Direc
raw_dir="raw_data"
FASTQC_OUT="processed_data/fastqc"
MULTIQC_OUT="processed_data/multiqc"


#Create Output Directories
echo "--> Creating output directories..."
mkdir -p "$FASTQC_OUT"
mkdir -p "$MULTIQC_OUT"

#Running the Quality Check
echo "Running FASTQC files in $raw_dir..."
fastqc "$raw_dir"/*.fastq.gz -o "$FASTQC_OUT"

echo "Running MultiQC aggregation..."
multiqc "$FASTQC_OUT" -o "$MULTIQC_OUT"

#  Cleanup of  fastqc folder since MultiQC has aggregated the data
echo "--> Deleting raw FastQC files to save disk space..."
rm -rf "$FASTQC_OUT"/*.zip

echo "Report available in $MULTIQC_OUT/multiqc_report.html"
```

This script was used to perform preliminary quality assessment of the raw sequencing reads. The workflow first initialized a dedicated Conda environment containing all required quality-control tools. The script defined the directories for the raw FASTQ files and the output folders used to store the FastQC and MultiQC results. FastQC was then executed on all .fastq.gz files in the raw-data directory to evaluate sequencing quality metrics, including per-base quality scores, GC content, duplication levels, and adapter contamination.

Following individual FastQC analyses, MultiQC was run to aggregate all FastQC outputs into a single comprehensive report, allowing efficient visualization and comparison of read-quality statistics across all samples. After aggregation, the temporary FastQC .zip files were removed to reduce disk usage. The final quality-control summary was saved as an HTML report generated by MultiQC.

Script 2 index construction:

```{bash}
#!/bin/bash
#SBATCH --partition=msc_appbio
#SBATCH --job-name=build_index
#SBATCH --output=logs/index_%j.output
#SBATCH --error=logs/index_%j.error
#SBATCH --nodes=1
#SBATCH --time=02:00:00
#SBATCH --mem=8G
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4

# Initialize Environment
eval "$(conda shell.bash hook)"
conda activate isobutanol_paper

#Set up Directories
REF_DIR="References"
ORIGINAL_FASTA="${REF_DIR}/S288C_reference_sequence_R64-1-1_20110203.fsa"
CLEAN_FASTA="${REF_DIR}/genome_clean.fa"
INDEX_DIR="${REF_DIR}/BowtieIndex"
INDEX_PREFIX="${INDEX_DIR}/genome"
ORIGINAL_GFF="${REF_DIR}/saccharomyces_cerevisiae_R64-1-1_20110208.gff"
NEW_GTF="${REF_DIR}/genes.gtf"

#Rename chromosomes in fasta
# This aligns the NCBI names (ref|NC_001133|) with SGD names (chrI)

echo "Renaming chromosomes in FASTA..."
if [ -f "$ORIGINAL_FASTA" ]; then
    sed 's/^>ref|NC_001133|.*/>chrI/' "$ORIGINAL_FASTA" | \
    sed 's/^>ref|NC_001134|.*/>chrII/' | \
    sed 's/^>ref|NC_001135|.*/>chrIII/' | \
    sed 's/^>ref|NC_001136|.*/>chrIV/' | \
    sed 's/^>ref|NC_001137|.*/>chrV/' | \
    sed 's/^>ref|NC_001138|.*/>chrVI/' | \
    sed 's/^>ref|NC_001139|.*/>chrVII/' | \
    sed 's/^>ref|NC_001140|.*/>chrVIII/' | \
    sed 's/^>ref|NC_001141|.*/>chrIX/' | \
    sed 's/^>ref|NC_001142|.*/>chrX/' | \
    sed 's/^>ref|NC_001143|.*/>chrXI/' | \
    sed 's/^>ref|NC_001144|.*/>chrXII/' | \
    sed 's/^>ref|NC_001145|.*/>chrXIII/' | \
    sed 's/^>ref|NC_001146|.*/>chrXIV/' | \
    sed 's/^>ref|NC_001147|.*/>chrXV/' | \
    sed 's/^>ref|NC_001148|.*/>chrXVI/' | \
    sed 's/^>ref|NC_001224|.*/>chrmt/' | \
    sed 's/^>ref|NC_001398|.*/>2-micron/' > "$CLEAN_FASTA"

    echo "Created clean FASTA at $CLEAN_FASTA"

#Build Index

echo "Building Bowtie2 index..."
mkdir -p "$INDEX_DIR"
bowtie2-build --threads 4 "$CLEAN_FASTA" "$INDEX_PREFIX"
echo "Index building complete."

#Conversion GFF to GTF
# Tophat requires GTF format. We use gffread to convert.
echo "Converting GFF to GTF..."

    gffread "$ORIGINAL_GFF" -T -o "$NEW_GTF"
    echo "Created GTF file at $NEW_GTF"

echo "All reference preparation tasks completed successfully."
```

This script initialized a dedicated Conda environment containing Bowtie2 and other required tools.

First, the script defined the directory structure and input files, including the original FASTA genome sequence and the corresponding GFF annotation file. Chromosome identifiers in the FASTA file were then standardized to match the Saccharomyces Genome Database (SGD) naming convention (e.g., chrI, chrII, …, chrXVI, and chrmt). This renaming step was performed using a series of sed commands and ensured consistent chromosome naming between the genome FASTA and gene-annotation files.

After generating the cleaned FASTA file, a Bowtie2 index was constructed using bowtie2-build with four CPU threads. The resulting index files were stored in a dedicated index directory for downstream alignment.

Finally, the script converted the original GFF annotation into GTF format using gffread, as some RNA-seq downstream tools require GTF rather than GFF. The resulting genes.gtf file and the Bowtie2 index together formed the complete reference package used for subsequent read alignment and quantification.

Script 3 read alignment:

```{bash}
#!/bin/bash
#SBATCH --partition=msc_appbio
#SBATCH --job-name=yeast_alignment
#SBATCH --output=logs/align_%j.output
#SBATCH --error=logs/align_%j.error
#SBATCH --nodes=1
#SBATCH --time=8:00:00
#SBATCH --mem=8G
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4

#Mapping rna-seq reads to reference genome via Tophat
eval "$(conda shell.bash hook)"
conda activate isobutanol_paper

#Directory setup
RAW_DIR="raw_data"
REF_INDEX="References/BowtieIndex/genome"
OUT_DIR="processed_data/tophat"
GTF_FILE="References/genes.gtf"
BAM_DIR="processed_data/bams"

mkdir -p "$OUT_DIR"
mkdir -p "$BAM_DIR"
mkdir -p logs

#Code logic
for file in "$RAW_DIR"/*_1.fastq.gz; do
filename=$(basename "$file") #Extracting the sample name
sample_id="${filename%_1.fastq.gz}"
echo "Processing sample : $sample_id"

#Defining filename
r1="${RAW_DIR}/${sample_id}_1.fastq.gz"
r2="${RAW_DIR}/${sample_id}_2.fastq.gz"

#Specific output dir for this result
sample_out="${OUT_DIR}/${sample_id}"
mkdir -p "$sample_out"

#Running the Tophat programme
tophat -p 4 \
       -G "$GTF_FILE" \
       -o "$sample_out" \
      "$REF_INDEX" \
      "$r1" "$r2"
echo  "Finished mapping $sample_id"

if [ -f "$sample_out/accepted_hits.bam" ]; then
        cp "$sample_out/accepted_hits.bam" "$BAM_DIR/${sample_id}.bam"
        echo "--> Saved consolidated BAM to $BAM_DIR/${sample_id}.bam"
fi

done

echo "Mapping of all samples completed"
```

Also，The script initialized a dedicated Conda environment containing Tophat and its dependencies. And, defined the input directories for raw FASTQ files, the Bowtie2 reference index, the gene-annotation GTF file, and the output directories for Tophat results and final BAM files. It then iterated through all paired-end read files in the raw-data directory by detecting files ending in _1.fastq.gz. For each sample, the script extracted the sample ID, located the corresponding read 1 and read 2 files, and created a sample-specific output folder.

Tophat was executed for each sample using four CPU threads, with the GTF annotation file provided to guide spliced alignment. The cleaned reference genome index generated earlier was used as the alignment target. Upon completion, Tophat produced the standard alignment file accepted_hits.bam.

If the BAM file was successfully generated, it was copied into a centralized directory (processed_data/bams/) using the sample’s name, ensuring consistent file organization for downstream analyses. This loop continued until all samples were processed, after which the script reported the completion of the alignment step.

Script 4 data processing:

```{bash}
#!/bin/bash
#SBATCH --partition=msc_appbio
#SBATCH --job-name=bam_processing
#SBATCH --output=logs/processing_%j.output
#SBATCH --error=logs/processing_%j.error
#SBATCH --nodes=1
#SBATCH --time=08:00:00
#SBATCH --mem=16G
#SBATCH --ntasks=1

#Set up enviorment 
eval "$(conda shell.bash hook)"
conda activate isobutanol_paper

#Define Directories
INPUT_DIR="processed_data/bams"
OUTPUT_DIR="processed_data/processed_bams"

mkdir -p "$OUTPUT_DIR"
mkdir -p logs

#Code logic
for bam_file in "$INPUT_DIR"/*.bam; do
filename=$(basename "$bam_file")
sample_id="${filename%.bam}"

echo "Processing Sample : $sample_id"

#Intermediate files for program
sorted_bam="${OUTPUT_DIR}/${sample_id}_sorted.bam"
dedup_bam="${OUTPUT_DIR}/${sample_id}_deduplicated.bam" 
#shows the deduplicated version, ready for quantification
metrics_file="${OUTPUT_DIR}/${sample_id}_metrics.txt"
#allows for log of the data removed post-processing 


#Sorting bam files via picard
picard SortSam \
I="$bam_file" \
O="$sorted_bam" \
SORT_ORDER=coordinate \
VALIDATION_STRINGENCY=LENIENT

#Removing dupes
picard MarkDuplicates \
I="$sorted_bam" \
O="$dedup_bam" \
M="$metrics_file" \
REMOVE_DUPLICATES=true \
VALIDATION_STRINGENCY=LENIENT
rm "$sorted_bam" #removing intermediate files needed 

echo "Finished $sample_id processing"

done

echo "All samples processed, located in $OUTPUT_DIR"
```

This script was used to perform standardized post-alignment processing of RNA-seq BAM files prior to downstream quantification. Job parameters were defined through the scheduler, and a Conda environment containing Picard was activated at runtime.

The script specified directories for input BAM files (generated from Tophat) and output processed files. For each BAM file within the input directory, the script extracted the sample identifier and generated filenames for sorted BAMs, deduplicated BAMs, and Picard metrics logs.

Each BAM file first underwent coordinate sorting using Picard's SortSam tool, producing a sorted BAM file suitable for further processing. Following sorting, duplicate reads were identified and removed using MarkDuplicates with the REMOVE_DUPLICATES=true option. A metrics file was generated for each sample to document duplication rates and provide traceability for removed reads. After deduplication, the intermediate sorted BAM file was deleted to conserve storage space.

The final deduplicated BAM files, suitable for quantification or other downstream analyses, were saved in the designated output directory. The script iterated over all input samples and reported completion upon processing all available BAM files.

Script 5 quantification:

```{bash}
#!/bin/bash
#SBATCH --partition=msc_appbio
#SBATCH --job-name=cufflinks_quant
#SBATCH --output=logs/quantification_%j.output
#SBATCH --error=logs/quantification_%j.error
#SBATCH --nodes=1
#SBATCH --time=08:00:00
#SBATCH --mem=8G
#SBATCH --ntasks=4

#Initialise conda env
eval "$(conda shell.bash hook)"
conda activate isobutanol_paper

#Set up Directories
INPUT_DIR="processed_data/processed_bams"
OUTPUT_DIR="processed_data/cufflinks"
REF_ANNOTATION="References/genes.gtf"
MASK_FILE="References/rRNA_tRNA_mask.gtf"

mkdir -p "$OUTPUT_DIR"
mkdir -p logs

# Creating mask file (Extracting rRNA and tRNA lines from the annotation file)
echo "Creating rRNA/tRNA mask file..."
grep -E 'rRNA|tRNA' "$REF_ANNOTATION" > "$MASK_FILE"

#Code logic
for bam_file in "$INPUT_DIR"/*_deduplicated.bam
do
    filename=$(basename "$bam_file") #Decouple bam suffix
    sample_id="${filename%_deduplicated.bam}"
    echo "Quantifying sample: $sample_id"

    sample_out="${OUTPUT_DIR}/${sample_id}"
    mkdir -p "$sample_out"

    #Cufflinks programme
    # Note: Using -g/-G with GFF files works in newer Cufflinks, 
    # but strictly GTF is preferred. If this fails, the GFF may need conversion.
    cufflinks -p 4 \
    -G "$REF_ANNOTATION" \
    -M "$MASK_FILE" \
    -o "$sample_out" \
    "$bam_file"

    echo "Finished $sample_id"
done

echo "Quantification step complete"
```

This script was used to generate transcript abundance estimates from the processed RNA-seq BAM files.  The Cufflinks suite was activated prior to quantification.

The script began by defining directory paths for the input BAM files, the output quantification directory, the reference GTF annotation file, and a mask file used to exclude non-coding RNA features. A mask file containing rRNA and tRNA annotations was created by extracting all corresponding entries from the reference GTF using a regular-expression search. This mask file prevents highly abundant rRNA/tRNA reads from inflating transcript-expression estimates.

For each deduplicated BAM file, the script extracted the sample identifier and created a dedicated output directory. Cufflinks was then executed with four threads, using the reference GTF file to guide transcript assembly and quantification. The -G option constrained the analysis to known gene models, and the -M option applied the rRNA/tRNA mask to exclude unwanted alignments. The resulting transcript FPKM estimates, along with associated tracking and model files, were written to the sample-specific output directory.

Script 6 differential expression analysis:

```{bash}
#!/bin/bash
#SBATCH --partition=msc_appbio
#SBATCH --job-name=cuffdiff_de
#SBATCH --output=logs/DE_analysis_%j.output
#SBATCH --error=logs/DE_analysis_%j.error
#SBATCH --nodes=1
#SBATCH --time=12:00:00
#SBATCH --mem=16G
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4

# Initialize Environment
eval "$(conda shell.bash hook)"
conda activate isobutanol_paper

# Set up Directories
BAM_DIR="processed_data/processed_bams"
OUTPUT_DIR="processed_data/cuffdiff"
REF_GTF="References/genes.gtf"
MASK_FILE="References/rRNA_tRNA_mask.gtf"

mkdir -p "$OUTPUT_DIR"
mkdir -p logs

# Creating mask file (with non-coding RNA added for robustness)
grep -iE 'rRNA|tRNA|snoRNA|snRNA|ncRNA' "$REF_GTF" > "$MASK_FILE"

# Files 94-97 had 0 expression of GLN3
GLN3_NO_ISO="${BAM_DIR}/ERR3450094_deduplicated.bam,${BAM_DIR}/ERR3450095_deduplicated.bam"
GLN3_ISO="${BAM_DIR}/ERR3450096_deduplicated.bam,${BAM_DIR}/ERR3450097_deduplicated.bam"

# WILD TYPE (WT)
# Files 98-101 had expression of GLN3
WT_NO_ISO="${BAM_DIR}/ERR3450098_deduplicated.bam,${BAM_DIR}/ERR3450099_deduplicated.bam"
WT_ISO="${BAM_DIR}/ERR3450100_deduplicated.bam,${BAM_DIR}/ERR3450101_deduplicated.bam"

echo "Starting differential expression analysis with cuffdiff"

# Run Cuffdiff
cuffdiff \
    -p 4 \
    -o "$OUTPUT_DIR" \
    -L gln3_NoIso,gln3_Iso,WT_NoIso,WT_Iso \
    -M "$MASK_FILE" \
    --library-type fr-unstranded \
    --no-update-check \
    "$REF_GTF" \
    "$GLN3_NO_ISO" "$GLN3_ISO" "$WT_NO_ISO" "$WT_ISO"

echo "Differential Expression Complete, results in $OUTPUT_DIR/gene_exp.diff"
```

This Script was conducted using Cuffdiff. Aligned, deduplicated BAM files were organized into four biological conditions (GLN3_NoIso, GLN3_Iso, WT_NoIso, WT_Iso), each containing replicates. To prevent inflated expression estimates from highly abundant non-coding RNAs, a mask file containing rRNA, tRNA, snoRNA, snRNA, and ncRNA annotations was generated and applied during analysis. Cuffdiff was run in reference-guided mode using the curated GTF annotation and an fr-unstranded library setting. The resulting differential expression statistics were reported in the default output files, including gene_exp.diff.